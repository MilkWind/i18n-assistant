"""
æŠ¥å‘Šç”Ÿæˆæ¨¡å— - ç”Ÿæˆåˆ†ææŠ¥å‘Šå’Œä¼˜åŒ–åçš„å›½é™…åŒ–æ–‡ä»¶
"""

import json
from dataclasses import asdict
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any

from .analyzer import AnalysisResult
from .config import Config
from .parser import ParseResult


class ReportGenerator:
    """æŠ¥å‘Šç”Ÿæˆå™¨"""

    def __init__(self, config: Config):
        self.config = config
        self.output_path = Path(config.output_path)
        self.output_path.mkdir(parents=True, exist_ok=True)
        self.session_dir = ""  # ä¼šè¯ç›®å½•

    def set_session_directory(self, session_dir: str) -> None:
        """è®¾ç½®ä¼šè¯ç›®å½•"""
        self.session_dir = session_dir

    def generate_full_report(self, analysis_result: AnalysisResult, parse_result: ParseResult) -> str:
        """
        ç”Ÿæˆå®Œæ•´çš„æ–‡æœ¬æŠ¥å‘Š
        
        Args:
            analysis_result: åˆ†æç»“æœ
            parse_result: è§£æç»“æœ
            
        Returns:
            str: æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
        """
        # ç¡®å®šæŠ¥å‘Šè¾“å‡ºè·¯å¾„
        if self.session_dir:
            reports_path = self.output_path / self.session_dir / "reports"
        else:
            reports_path = self.output_path / "reports"

        reports_path.mkdir(parents=True, exist_ok=True)

        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        report_lines = []

        # æŠ¥å‘Šå¤´éƒ¨
        report_lines.extend(
            ["=" * 60, "å›½é™…åŒ–åˆ†ææŠ¥å‘Š", "=" * 60, f"ç”Ÿæˆæ—¶é—´: {timestamp}", f"é¡¹ç›®è·¯å¾„: {self.config.project_path}",
             f"å›½é™…åŒ–ç›®å½•: {self.config.i18n_path}", f"è¾“å‡ºç›®å½•: {self.config.output_path}", "", ])

        # æ¦‚è§ˆç»Ÿè®¡
        report_lines.extend(["=" * 60, "1. æ¦‚è§ˆç»Ÿè®¡", "=" * 60, f"æ€»ä½¿ç”¨é”®æ•°: {analysis_result.total_used_keys}",
                             f"æ€»å®šä¹‰é”®æ•°: {analysis_result.total_defined_keys}",
                             f"åŒ¹é…é”®æ•°: {analysis_result.matched_keys}",
                             f"è¦†ç›–ç‡: {analysis_result.coverage_percentage:.2f}%",
                             f"ç¼ºå¤±é”®æ•°: {len(analysis_result.missing_keys)}",
                             f"æœªä½¿ç”¨é”®æ•°: {len(analysis_result.unused_keys)}",
                             f"ä¸ä¸€è‡´é”®æ•°: {len(analysis_result.inconsistent_keys)}", "", ])

        # ç¼ºå¤±çš„å›½é™…åŒ–æ–‡æœ¬
        if analysis_result.missing_keys:
            report_lines.extend(["=" * 60, "2. ç¼ºå¤±çš„å›½é™…åŒ–æ–‡æœ¬", "=" * 60, ])

            # æŒ‰æ–‡ä»¶åˆ†ç»„æ˜¾ç¤ºç»Ÿè®¡æ¦‚è§ˆ
            missing_keys_by_file = getattr(analysis_result, 'missing_keys_by_file', {})
            if missing_keys_by_file:
                report_lines.extend(["", "æ–‡ä»¶ç»Ÿè®¡æ¦‚è§ˆ:", "-" * 30, ])
                for file_path, missing_list in missing_keys_by_file.items():
                    report_lines.append(f"  {file_path}: {len(missing_list)} ä¸ªç¼ºå¤±é”®")

            report_lines.extend(["", "è¯¦ç»†åˆ—è¡¨:", "-" * 30, ])

            # æŒ‰æ–‡ä»¶åˆ†ç»„æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            missing_by_file = {}
            for missing in analysis_result.missing_keys:
                if missing.file_path not in missing_by_file:
                    missing_by_file[missing.file_path] = []
                missing_by_file[missing.file_path].append(missing)

            for file_path, missing_list in missing_by_file.items():
                report_lines.append(f"\næ–‡ä»¶: {file_path}")
                report_lines.append("-" * 40)
                for missing in missing_list:
                    report_lines.append(f"  è¡Œ {missing.line_number}: '{missing.key}'")
                    if missing.suggested_files:
                        suggestions = ", ".join(missing.suggested_files)
                        report_lines.append(f"    å»ºè®®æ·»åŠ åˆ°: {suggestions}")
        else:
            report_lines.extend(["=" * 60, "2. ç¼ºå¤±çš„å›½é™…åŒ–æ–‡æœ¬", "=" * 60, "âœ… æ²¡æœ‰å‘ç°ç¼ºå¤±çš„å›½é™…åŒ–æ–‡æœ¬ï¼", "", ])

        # æœªä½¿ç”¨çš„å›½é™…åŒ–æ–‡æœ¬
        if analysis_result.unused_keys:
            report_lines.extend(["", "=" * 60, "3. æœªä½¿ç”¨çš„å›½é™…åŒ–æ–‡æœ¬", "=" * 60, ])

            # æŒ‰æ–‡ä»¶åˆ†ç»„æ˜¾ç¤ºç»Ÿè®¡æ¦‚è§ˆ
            unused_keys_by_file = getattr(analysis_result, 'unused_keys_by_file', {})
            if unused_keys_by_file:
                report_lines.extend(["", "æ–‡ä»¶ç»Ÿè®¡æ¦‚è§ˆ:", "-" * 30, ])
                for file_path, unused_list in unused_keys_by_file.items():
                    report_lines.append(f"  {file_path}: {len(unused_list)} ä¸ªæœªä½¿ç”¨é”®")

            report_lines.extend(["", "è¯¦ç»†åˆ—è¡¨:", "-" * 30, ])

            # æŒ‰æ–‡ä»¶åˆ†ç»„æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            unused_by_file = {}
            for unused in analysis_result.unused_keys:
                if unused.i18n_file not in unused_by_file:
                    unused_by_file[unused.i18n_file] = []
                unused_by_file[unused.i18n_file].append(unused)

            for file_path, unused_list in unused_by_file.items():
                report_lines.append(f"\næ–‡ä»¶: {file_path}")
                report_lines.append("-" * 40)
                for unused in unused_list:
                    report_lines.append(f"  '{unused.key}': {unused.value}")
        else:
            report_lines.extend(
                ["", "=" * 60, "3. æœªä½¿ç”¨çš„å›½é™…åŒ–æ–‡æœ¬", "=" * 60, "âœ… æ²¡æœ‰å‘ç°æœªä½¿ç”¨çš„å›½é™…åŒ–æ–‡æœ¬ï¼", "", ])

        # ä¸ä¸€è‡´çš„å›½é™…åŒ–å­—æ®µ
        if analysis_result.inconsistent_keys:
            report_lines.extend(["", "=" * 60, "4. ä¸ä¸€è‡´çš„å›½é™…åŒ–å­—æ®µ", "=" * 60, ])

            for inconsistent in analysis_result.inconsistent_keys:
                report_lines.append(f"\né”®: '{inconsistent.key}'")
                report_lines.append(f"  å­˜åœ¨äº: {', '.join(inconsistent.existing_files)}")
                report_lines.append(f"  ç¼ºå¤±äº: {', '.join(inconsistent.missing_files)}")
        else:
            report_lines.extend(
                ["", "=" * 60, "4. ä¸ä¸€è‡´çš„å›½é™…åŒ–å­—æ®µ", "=" * 60, "âœ… æ²¡æœ‰å‘ç°ä¸ä¸€è‡´çš„å›½é™…åŒ–å­—æ®µï¼", "", ])

        # å˜é‡æ’å€¼çš„å›½é™…åŒ–è°ƒç”¨  
        if analysis_result.variable_interpolation_calls:
            report_lines.extend(["", "=" * 60, "5. å˜é‡æ’å€¼çš„å›½é™…åŒ–è°ƒç”¨", "=" * 60, ])

            # æŒ‰æ–‡ä»¶ç»Ÿè®¡æ¦‚è§ˆ
            variable_interpolation_by_file = analysis_result.variable_interpolation_by_file
            if variable_interpolation_by_file:
                report_lines.extend(["", "æ–‡ä»¶ç»Ÿè®¡æ¦‚è§ˆ:", "-" * 30, ])
                for file_path, vi_list in variable_interpolation_by_file.items():
                    report_lines.append(f"  {file_path}: {len(vi_list)} ä¸ªå˜é‡æ’å€¼è°ƒç”¨")

            report_lines.extend(["", "è¯¦ç»†åˆ—è¡¨:", "-" * 30, ])

            # æŒ‰æ–‡ä»¶åˆ†ç»„æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            for file_path, vi_list in variable_interpolation_by_file.items():
                report_lines.append(f"\næ–‡ä»¶: {file_path}")
                report_lines.append("-" * 40)
                for vi_call in vi_list:
                    report_lines.append(f"  è¡Œ {vi_call.line_number}: {vi_call.match_text}")
                    report_lines.append(f"    é”®æ¨¡å¼: '{vi_call.key}'")

            report_lines.extend(
                ["", "âš ï¸  æ³¨æ„äº‹é¡¹:", "-" * 30, "  è¿™äº›è°ƒç”¨ä½¿ç”¨äº†å˜é‡æ’å€¼ï¼Œå¯èƒ½åœ¨è¿è¡Œæ—¶åŠ¨æ€ç”Ÿæˆå…·ä½“çš„é”®å€¼ã€‚",
                 "  åœ¨åˆ é™¤æœªä½¿ç”¨çš„å›½é™…åŒ–é”®æ—¶ï¼Œè¯·æ£€æŸ¥è¿™äº›æ¨¡å¼æ˜¯å¦å¯èƒ½åŒ¹é…åˆ°æ‚¨è¦åˆ é™¤çš„é”®ã€‚",
                 "  ä¾‹å¦‚ï¼št(`words.${pos}`) å¯èƒ½ä¼šåŒ¹é… words.0, words.1, words.home ç­‰é”®ã€‚",
                 "  å»ºè®®åœ¨åˆ é™¤é”®ä¹‹å‰ï¼Œä»”ç»†æ£€æŸ¥ä¼˜åŒ–åçš„æ–‡ä»¶æ˜¯å¦è¯¯åˆ äº†è¿™äº›åŠ¨æ€å¼•ç”¨çš„é”®ã€‚", ""])
        else:
            report_lines.extend(
                ["", "=" * 60, "5. å˜é‡æ’å€¼çš„å›½é™…åŒ–è°ƒç”¨", "=" * 60, "âœ… æ²¡æœ‰å‘ç°å˜é‡æ’å€¼çš„å›½é™…åŒ–è°ƒç”¨ã€‚", "", ])

        # æ–‡ä»¶è¦†ç›–æƒ…å†µ
        if analysis_result.file_coverage:
            report_lines.extend(["", "=" * 60, "6. æ–‡ä»¶è¦†ç›–æƒ…å†µ", "=" * 60, ])

            for file_path, coverage in analysis_result.file_coverage.items():
                report_lines.extend([f"\næ–‡ä»¶: {file_path}", f"  æ€»è°ƒç”¨æ•°: {coverage.total_calls}",
                                     f"  è¦†ç›–è°ƒç”¨æ•°: {coverage.covered_calls}",
                                     f"  è¦†ç›–ç‡: {coverage.coverage_percentage:.2f}%", ])

        # å»ºè®®éƒ¨åˆ†
        suggestions = self._generate_suggestions(analysis_result)
        if suggestions:
            report_lines.extend(["", "=" * 60, "7. æ”¹è¿›å»ºè®®", "=" * 60, ])
            report_lines.extend(suggestions)

        # æŠ¥å‘Šå°¾éƒ¨
        report_lines.extend(["", "=" * 60, "æŠ¥å‘Šç»“æŸ", "=" * 60, ])

        # å†™å…¥æ–‡ä»¶
        report_content = "\n".join(report_lines)
        report_file = reports_path / "analysis_report.txt"

        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)

        return str(report_file)

    def generate_text_report(self, analysis_result: AnalysisResult) -> str:
        """
        ç”Ÿæˆç®€åŒ–çš„æ–‡æœ¬æŠ¥å‘Š
        
        Args:
            analysis_result: åˆ†æç»“æœ
            
        Returns:
            str: æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
        """
        # ç¡®å®šæŠ¥å‘Šè¾“å‡ºè·¯å¾„
        if self.session_dir:
            reports_path = self.output_path / self.session_dir / "reports"
        else:
            reports_path = self.output_path / "reports"

        reports_path.mkdir(parents=True, exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        summary = self.generate_summary_report(analysis_result)

        # å†™å…¥æ–‡ä»¶
        report_file = reports_path / f"summary_report_{timestamp}.txt"

        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(summary)

        return str(report_file)

    def generate_json_report(self, analysis_result: AnalysisResult) -> str:
        """
        ç”ŸæˆJSONæ ¼å¼çš„æŠ¥å‘Š
        
        Args:
            analysis_result: åˆ†æç»“æœ
            
        Returns:
            str: æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
        """
        # ç¡®å®šæŠ¥å‘Šè¾“å‡ºè·¯å¾„
        if self.session_dir:
            reports_path = self.output_path / self.session_dir / "reports"
        else:
            reports_path = self.output_path / "reports"

        reports_path.mkdir(parents=True, exist_ok=True)

        # æ„å»ºæŠ¥å‘Šæ•°æ®
        report_data = {'timestamp': datetime.now().isoformat(),
                       'summary': {'total_used_keys': analysis_result.total_used_keys,
                                   'total_defined_keys': analysis_result.total_defined_keys,
                                   'matched_keys': analysis_result.matched_keys,
                                   'coverage_percentage': analysis_result.coverage_percentage,
                                   'variable_interpolation_count': len(analysis_result.variable_interpolation_calls)},
                       'missing_keys': [asdict(mk) for mk in analysis_result.missing_keys],
                       'missing_keys_by_file': {file_path: [asdict(mk) for mk in missing_list] for
                                                file_path, missing_list in
                                                getattr(analysis_result, 'missing_keys_by_file', {}).items()},
                       'missing_keys_summary_by_file': getattr(analysis_result, 'get_missing_keys_summary_by_file',
                                                               lambda: {})(),
                       'unused_keys': [asdict(uk) for uk in analysis_result.unused_keys],
                       'unused_keys_by_file': {file_path: [asdict(uk) for uk in unused_list] for file_path, unused_list
                                               in getattr(analysis_result, 'unused_keys_by_file', {}).items()},
                       'unused_keys_summary_by_file': getattr(analysis_result, 'get_unused_keys_summary_by_file',
                                                              lambda: {})(),
                       'inconsistent_keys': [asdict(ik) for ik in analysis_result.inconsistent_keys],
                       'variable_interpolation_calls': [asdict(vi) for vi in
                                                        analysis_result.variable_interpolation_calls],
                       'variable_interpolation_by_file': {file_path: [asdict(vi) for vi in vi_list] for
                                                          file_path, vi_list in
                                                          analysis_result.variable_interpolation_by_file.items()},
                       'variable_interpolation_summary_by_file': getattr(analysis_result,
                                                                         'get_variable_interpolation_summary_by_file',
                                                                         lambda: {})(),
                       'file_coverage': {file_path: asdict(coverage) for file_path, coverage in
                                         analysis_result.file_coverage.items()}}

        # å†™å…¥JSONæ–‡ä»¶
        json_file = reports_path / "analysis_report.json"

        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)

        return str(json_file)

    def generate_missing_keys_template(self, analysis_result: AnalysisResult) -> List[str]:
        """
        ç”Ÿæˆç¼ºå¤±é”®çš„æ¨¡æ¿æ–‡ä»¶
        
        Args:
            analysis_result: åˆ†æç»“æœ
            
        Returns:
            List[str]: ç”Ÿæˆçš„æ¨¡æ¿æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        if not analysis_result.missing_keys:
            return []

        generated_files = []

        # æŒ‰å»ºè®®çš„æ–‡ä»¶åˆ†ç»„ç¼ºå¤±çš„é”®
        keys_by_suggested_file = {}
        for missing in analysis_result.missing_keys:
            for suggested_file in missing.suggested_files:
                if suggested_file not in keys_by_suggested_file:
                    keys_by_suggested_file[suggested_file] = set()
                keys_by_suggested_file[suggested_file].add(missing.key)

        # ä¸ºæ¯ä¸ªå»ºè®®æ–‡ä»¶ç”Ÿæˆæ¨¡æ¿
        for suggested_file, keys in keys_by_suggested_file.items():
            template_data = {}

            # ç”Ÿæˆæ¨¡æ¿ç»“æ„
            for key in sorted(keys):
                self._set_nested_value(template_data, key, f"TODO: Add translation for '{key}'")

            # åˆ›å»ºæ¨¡æ¿æ–‡ä»¶
            original_path = Path(suggested_file)
            template_file = self.output_path / "templates" / f"missing_keys_{original_path.stem}.json"
            template_file.parent.mkdir(parents=True, exist_ok=True)

            with open(template_file, 'w', encoding='utf-8') as f:
                json.dump(template_data, f, ensure_ascii=False, indent=2)

            generated_files.append(str(template_file))

        return generated_files

    def _generate_suggestions(self, analysis_result: AnalysisResult) -> List[str]:
        """ç”Ÿæˆåˆ†æå»ºè®®"""
        suggestions = []

        if analysis_result.missing_keys:
            suggestions.extend(
                [f"â€¢ å‘ç° {len(analysis_result.missing_keys)} ä¸ªç¼ºå¤±çš„å›½é™…åŒ–é”®ï¼Œå»ºè®®åŠæ—¶æ·»åŠ åˆ°ç›¸åº”çš„i18næ–‡ä»¶ä¸­",
                 "â€¢ å¯ä»¥ä½¿ç”¨ç”Ÿæˆçš„æ¨¡æ¿æ–‡ä»¶æ¥å¿«é€Ÿæ·»åŠ ç¼ºå¤±çš„é”®"])

        if analysis_result.unused_keys:
            suggestions.extend(
                [f"â€¢ å‘ç° {len(analysis_result.unused_keys)} ä¸ªæœªä½¿ç”¨çš„å›½é™…åŒ–é”®ï¼Œå¯ä»¥è€ƒè™‘åˆ é™¤ä»¥å‡å°‘æ–‡ä»¶å¤§å°",
                 "â€¢ å·²ç”Ÿæˆä¼˜åŒ–åçš„i18næ–‡ä»¶ï¼Œç§»é™¤äº†æœªä½¿ç”¨çš„é”®"])

        if analysis_result.inconsistent_keys:
            suggestions.extend([f"â€¢ å‘ç° {len(analysis_result.inconsistent_keys)} ä¸ªä¸ä¸€è‡´çš„å›½é™…åŒ–é”®",
                                "â€¢ å»ºè®®ä¿æŒæ‰€æœ‰è¯­è¨€æ–‡ä»¶çš„é”®ç»“æ„ä¸€è‡´"])

        # è¦†ç›–ç‡å»ºè®®
        if analysis_result.coverage_percentage < 80:
            suggestions.append("â€¢ å½“å‰è¦†ç›–ç‡è¾ƒä½ï¼Œå»ºè®®ä¼˜å…ˆå¤„ç†ç¼ºå¤±çš„é”®")
        elif analysis_result.coverage_percentage < 95:
            suggestions.append("â€¢ è¦†ç›–ç‡è‰¯å¥½ï¼Œå»ºè®®å®Œå–„å‰©ä½™çš„ç¼ºå¤±é”®")
        else:
            suggestions.append("â€¢ è¦†ç›–ç‡ä¼˜ç§€ï¼å»ºè®®å®šæœŸæ£€æŸ¥ä¿æŒä»£ç è´¨é‡")

        if not suggestions:
            suggestions.append("â€¢ é¡¹ç›®çš„å›½é™…åŒ–çŠ¶å†µè‰¯å¥½ï¼Œæ²¡æœ‰å‘ç°æ˜æ˜¾é—®é¢˜ï¼")

        return suggestions

    def _set_nested_value(self, data: Dict[str, Any], key_path: str, value: Any):
        """è®¾ç½®åµŒå¥—é”®å€¼"""
        keys = key_path.split('.')
        current = data

        for key in keys[:-1]:
            if key not in current:
                current[key] = {}
            current = current[key]

        current[keys[-1]] = value

    def generate_summary_report(self, analysis_result: AnalysisResult) -> str:
        """
        ç”Ÿæˆç®€è¦æ‘˜è¦æŠ¥å‘Š
        
        Args:
            analysis_result: åˆ†æç»“æœ
            
        Returns:
            str: æ‘˜è¦æŠ¥å‘Šå†…å®¹
        """
        summary_lines = ["=" * 40, "å›½é™…åŒ–åˆ†ææ‘˜è¦", "=" * 40, f"ğŸ“Š è¦†ç›–ç‡: {analysis_result.coverage_percentage:.1f}%",
                         f"âœ… åŒ¹é…é”®: {analysis_result.matched_keys}/{analysis_result.total_used_keys}",
                         f"âŒ ç¼ºå¤±é”®: {len(analysis_result.missing_keys)}",
                         f"ğŸ—‘ï¸ æœªä½¿ç”¨é”®: {len(analysis_result.unused_keys)}",
                         f"âš ï¸ ä¸ä¸€è‡´é”®: {len(analysis_result.inconsistent_keys)}",
                         f"ğŸ”— å˜é‡æ’å€¼è°ƒç”¨: {len(analysis_result.variable_interpolation_calls)}", ]

        # æ·»åŠ æŒ‰æ–‡ä»¶ç»Ÿè®¡çš„æœªä½¿ç”¨é”®æ‘˜è¦
        unused_keys_by_file = getattr(analysis_result, 'unused_keys_by_file', {})
        if unused_keys_by_file:
            summary_lines.extend(["", "ğŸ“‚ æœªä½¿ç”¨é”®æŒ‰æ–‡ä»¶ç»Ÿè®¡:", ])
            for file_path, unused_list in unused_keys_by_file.items():
                file_name = file_path.split('/')[-1] if '/' in file_path else file_path.split('\\')[-1]
                summary_lines.append(f"   {file_name}: {len(unused_list)} ä¸ª")

        # æ·»åŠ æŒ‰æ–‡ä»¶ç»Ÿè®¡çš„å˜é‡æ’å€¼è°ƒç”¨æ‘˜è¦
        variable_interpolation_by_file = getattr(analysis_result, 'variable_interpolation_by_file', {})
        if variable_interpolation_by_file:
            summary_lines.extend(["", "ğŸ”— å˜é‡æ’å€¼è°ƒç”¨æŒ‰æ–‡ä»¶ç»Ÿè®¡:", ])
            for file_path, vi_list in variable_interpolation_by_file.items():
                file_name = file_path.split('/')[-1] if '/' in file_path else file_path.split('\\')[-1]
                summary_lines.append(f"   {file_name}: {len(vi_list)} ä¸ª")

        summary_lines.append("=" * 40)

        return "\n".join(summary_lines)

"""
æŠ¥å‘Šç”Ÿæˆæ¨¡å— - ç”Ÿæˆåˆ†ææŠ¥å‘Šå’Œä¼˜åŒ–åçš„å›½é™…åŒ–æ–‡ä»¶
"""

import json
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
from dataclasses import asdict

from .analyzer import AnalysisResult, MissingKey, UnusedKey, InconsistentKey
from .parser import ParseResult
from .config import Config


class ReportGenerator:
    """æŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self, config: Config):
        self.config = config
        self.output_path = Path(config.output_path)
        self.output_path.mkdir(parents=True, exist_ok=True)
    
    def generate_full_report(
        self, 
        analysis_result: AnalysisResult, 
        parse_result: ParseResult
    ) -> str:
        """
        ç”Ÿæˆå®Œæ•´çš„æ–‡æœ¬æŠ¥å‘Š
        
        Args:
            analysis_result: åˆ†æç»“æœ
            parse_result: è§£æç»“æœ
            
        Returns:
            str: æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
        """
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        report_lines = []
        
        # æŠ¥å‘Šå¤´éƒ¨
        report_lines.extend([
            "=" * 60,
            "å›½é™…åŒ–åˆ†ææŠ¥å‘Š",
            "=" * 60,
            f"ç”Ÿæˆæ—¶é—´: {timestamp}",
            f"é¡¹ç›®è·¯å¾„: {self.config.project_path}",
            f"å›½é™…åŒ–ç›®å½•: {self.config.i18n_path}",
            f"è¾“å‡ºç›®å½•: {self.config.output_path}",
            "",
        ])
        
        # æ¦‚è§ˆç»Ÿè®¡
        report_lines.extend([
            "=" * 60,
            "1. æ¦‚è§ˆç»Ÿè®¡",
            "=" * 60,
            f"æ€»ä½¿ç”¨é”®æ•°: {analysis_result.total_used_keys}",
            f"æ€»å®šä¹‰é”®æ•°: {analysis_result.total_defined_keys}",
            f"åŒ¹é…é”®æ•°: {analysis_result.matched_keys}",
            f"è¦†ç›–ç‡: {analysis_result.coverage_percentage:.2f}%",
            f"ç¼ºå¤±é”®æ•°: {len(analysis_result.missing_keys)}",
            f"æœªä½¿ç”¨é”®æ•°: {len(analysis_result.unused_keys)}",
            f"ä¸ä¸€è‡´é”®æ•°: {len(analysis_result.inconsistent_keys)}",
            "",
        ])
        
        # ç¼ºå¤±çš„å›½é™…åŒ–æ–‡æœ¬
        if analysis_result.missing_keys:
            report_lines.extend([
                "=" * 60,
                "2. ç¼ºå¤±çš„å›½é™…åŒ–æ–‡æœ¬",
                "=" * 60,
            ])
            
            # æŒ‰æ–‡ä»¶åˆ†ç»„æ˜¾ç¤º
            missing_by_file = {}
            for missing in analysis_result.missing_keys:
                if missing.file_path not in missing_by_file:
                    missing_by_file[missing.file_path] = []
                missing_by_file[missing.file_path].append(missing)
            
            for file_path, missing_list in missing_by_file.items():
                report_lines.append(f"\næ–‡ä»¶: {file_path}")
                report_lines.append("-" * 40)
                for missing in missing_list:
                    report_lines.append(f"  è¡Œ {missing.line_number}: '{missing.key}'")
                    if missing.suggested_files:
                        suggestions = ", ".join(missing.suggested_files)
                        report_lines.append(f"    å»ºè®®æ·»åŠ åˆ°: {suggestions}")
        else:
            report_lines.extend([
                "=" * 60,
                "2. ç¼ºå¤±çš„å›½é™…åŒ–æ–‡æœ¬",
                "=" * 60,
                "âœ… æ²¡æœ‰å‘ç°ç¼ºå¤±çš„å›½é™…åŒ–æ–‡æœ¬ï¼",
                "",
            ])
        
        # æœªä½¿ç”¨çš„å›½é™…åŒ–æ–‡æœ¬
        if analysis_result.unused_keys:
            report_lines.extend([
                "",
                "=" * 60,
                "3. æœªä½¿ç”¨çš„å›½é™…åŒ–æ–‡æœ¬",
                "=" * 60,
            ])
            
            # æŒ‰æ–‡ä»¶åˆ†ç»„æ˜¾ç¤º
            unused_by_file = {}
            for unused in analysis_result.unused_keys:
                if unused.i18n_file not in unused_by_file:
                    unused_by_file[unused.i18n_file] = []
                unused_by_file[unused.i18n_file].append(unused)
            
            for file_path, unused_list in unused_by_file.items():
                report_lines.append(f"\næ–‡ä»¶: {file_path}")
                report_lines.append("-" * 40)
                for unused in unused_list:
                    report_lines.append(f"  '{unused.key}': {unused.value}")
        else:
            report_lines.extend([
                "",
                "=" * 60,
                "3. æœªä½¿ç”¨çš„å›½é™…åŒ–æ–‡æœ¬",
                "=" * 60,
                "âœ… æ²¡æœ‰å‘ç°æœªä½¿ç”¨çš„å›½é™…åŒ–æ–‡æœ¬ï¼",
                "",
            ])
        
        # ä¸ä¸€è‡´çš„å›½é™…åŒ–å­—æ®µ
        if analysis_result.inconsistent_keys:
            report_lines.extend([
                "",
                "=" * 60,
                "4. ä¸ä¸€è‡´çš„å›½é™…åŒ–å­—æ®µ",
                "=" * 60,
            ])
            
            for inconsistent in analysis_result.inconsistent_keys:
                report_lines.append(f"\né”®: '{inconsistent.key}'")
                report_lines.append(f"  å­˜åœ¨äº: {', '.join(inconsistent.existing_files)}")
                report_lines.append(f"  ç¼ºå¤±äº: {', '.join(inconsistent.missing_files)}")
        else:
            report_lines.extend([
                "",
                "=" * 60,
                "4. ä¸ä¸€è‡´çš„å›½é™…åŒ–å­—æ®µ",
                "=" * 60,
                "âœ… æ²¡æœ‰å‘ç°ä¸ä¸€è‡´çš„å›½é™…åŒ–å­—æ®µï¼",
                "",
            ])
        
        # æ–‡ä»¶è¦†ç›–æƒ…å†µ
        if analysis_result.file_coverage:
            report_lines.extend([
                "",
                "=" * 60,
                "5. æ–‡ä»¶è¦†ç›–æƒ…å†µ",
                "=" * 60,
            ])
            
            for file_path, coverage in analysis_result.file_coverage.items():
                report_lines.extend([
                    f"\næ–‡ä»¶: {file_path}",
                    f"  æ€»è°ƒç”¨æ•°: {coverage.total_calls}",
                    f"  å·²è¦†ç›–: {coverage.covered_calls}",
                    f"  æœªè¦†ç›–: {coverage.uncovered_calls}",
                    f"  è¦†ç›–ç‡: {coverage.coverage_percentage:.2f}%",
                ])
        
        # åˆ†æå»ºè®®
        report_lines.extend([
            "",
            "=" * 60,
            "6. åˆ†æå»ºè®®",
            "=" * 60,
        ])
        
        suggestions = self._generate_suggestions(analysis_result)
        report_lines.extend(suggestions)
        
        # å†™å…¥æŠ¥å‘Šæ–‡ä»¶
        report_content = "\n".join(report_lines)
        report_file = self.output_path / f"analysis_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        return str(report_file)
    
    def generate_text_report(self, analysis_result: AnalysisResult) -> str:
        """
        ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š (åˆ«åæ–¹æ³•ï¼Œä¸ºäº†å…¼å®¹æ€§)
        
        Args:
            analysis_result: åˆ†æç»“æœ
            
        Returns:
            str: æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
        """
        # This is an alias for generate_full_report for backward compatibility
        from .parser import ParseResult
        # Create a minimal ParseResult for compatibility
        parse_result = ParseResult(
            files=[],
            total_keys=set(),
            duplicate_keys={},
            inconsistent_keys={},
            parse_errors=[]
        )
        return self.generate_full_report(analysis_result, parse_result)
    
    def generate_json_report(self, analysis_result: AnalysisResult) -> str:
        """
        ç”ŸæˆJSONæ ¼å¼çš„æŠ¥å‘Š
        
        Args:
            analysis_result: åˆ†æç»“æœ
            
        Returns:
            str: JSONæŠ¥å‘Šæ–‡ä»¶è·¯å¾„
        """
        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸
        report_data = {
            'timestamp': datetime.now().isoformat(),
            'config': {
                'project_path': self.config.project_path,
                'i18n_path': self.config.i18n_path,
                'output_path': self.config.output_path
            },
            'statistics': {
                'total_used_keys': analysis_result.total_used_keys,
                'total_defined_keys': analysis_result.total_defined_keys,
                'matched_keys': analysis_result.matched_keys,
                'coverage_percentage': round(analysis_result.coverage_percentage, 2),
                'missing_keys_count': len(analysis_result.missing_keys),
                'unused_keys_count': len(analysis_result.unused_keys),
                'inconsistent_keys_count': len(analysis_result.inconsistent_keys)
            },
            'missing_keys': [asdict(mk) for mk in analysis_result.missing_keys],
            'unused_keys': [asdict(uk) for uk in analysis_result.unused_keys],
            'inconsistent_keys': [asdict(ik) for ik in analysis_result.inconsistent_keys],
            'file_coverage': {
                file_path: asdict(coverage) 
                for file_path, coverage in analysis_result.file_coverage.items()
            }
        }
        
        # å†™å…¥JSONæ–‡ä»¶
        json_file = self.output_path / f"analysis_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)
        
        return str(json_file)
    
    def generate_optimized_i18n_files(
        self, 
        analysis_result: AnalysisResult, 
        parse_result: ParseResult
    ) -> List[str]:
        """
        ç”Ÿæˆä¼˜åŒ–åçš„å›½é™…åŒ–æ–‡ä»¶ï¼ˆç§»é™¤æœªä½¿ç”¨çš„é”®ï¼‰
        
        Args:
            analysis_result: åˆ†æç»“æœ
            parse_result: è§£æç»“æœ
            
        Returns:
            List[str]: ç”Ÿæˆçš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        generated_files = []
        
        # æ”¶é›†æœªä½¿ç”¨çš„é”®
        unused_keys = {uk.key for uk in analysis_result.unused_keys}
        
        # ä¸ºæ¯ä¸ªåŸå§‹i18næ–‡ä»¶ç”Ÿæˆä¼˜åŒ–ç‰ˆæœ¬
        for file_path, file_data in parse_result.files_data.items():
            optimized_data = self._remove_unused_keys(file_data, unused_keys)
            
            # åˆ›å»ºè¾“å‡ºæ–‡ä»¶è·¯å¾„
            original_path = Path(file_path)
            output_file = self.output_path / "optimized" / original_path.name
            output_file.parent.mkdir(parents=True, exist_ok=True)
            
            # å†™å…¥ä¼˜åŒ–åçš„æ–‡ä»¶
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(optimized_data, f, ensure_ascii=False, indent=2)
            
            generated_files.append(str(output_file))
        
        return generated_files
    
    def generate_missing_keys_template(self, analysis_result: AnalysisResult) -> List[str]:
        """
        ç”Ÿæˆç¼ºå¤±é”®çš„æ¨¡æ¿æ–‡ä»¶
        
        Args:
            analysis_result: åˆ†æç»“æœ
            
        Returns:
            List[str]: ç”Ÿæˆçš„æ¨¡æ¿æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        if not analysis_result.missing_keys:
            return []
        
        generated_files = []
        
        # æŒ‰å»ºè®®çš„æ–‡ä»¶åˆ†ç»„ç¼ºå¤±çš„é”®
        keys_by_suggested_file = {}
        for missing in analysis_result.missing_keys:
            for suggested_file in missing.suggested_files:
                if suggested_file not in keys_by_suggested_file:
                    keys_by_suggested_file[suggested_file] = set()
                keys_by_suggested_file[suggested_file].add(missing.key)
        
        # ä¸ºæ¯ä¸ªå»ºè®®æ–‡ä»¶ç”Ÿæˆæ¨¡æ¿
        for suggested_file, keys in keys_by_suggested_file.items():
            template_data = {}
            
            # ç”Ÿæˆæ¨¡æ¿ç»“æ„
            for key in sorted(keys):
                self._set_nested_value(template_data, key, f"TODO: Add translation for '{key}'")
            
            # åˆ›å»ºæ¨¡æ¿æ–‡ä»¶
            original_path = Path(suggested_file)
            template_file = self.output_path / "templates" / f"missing_keys_{original_path.stem}.json"
            template_file.parent.mkdir(parents=True, exist_ok=True)
            
            with open(template_file, 'w', encoding='utf-8') as f:
                json.dump(template_data, f, ensure_ascii=False, indent=2)
            
            generated_files.append(str(template_file))
        
        return generated_files
    
    def _generate_suggestions(self, analysis_result: AnalysisResult) -> List[str]:
        """ç”Ÿæˆåˆ†æå»ºè®®"""
        suggestions = []
        
        if analysis_result.missing_keys:
            suggestions.extend([
                f"â€¢ å‘ç° {len(analysis_result.missing_keys)} ä¸ªç¼ºå¤±çš„å›½é™…åŒ–é”®ï¼Œå»ºè®®åŠæ—¶æ·»åŠ åˆ°ç›¸åº”çš„i18næ–‡ä»¶ä¸­",
                "â€¢ å¯ä»¥ä½¿ç”¨ç”Ÿæˆçš„æ¨¡æ¿æ–‡ä»¶æ¥å¿«é€Ÿæ·»åŠ ç¼ºå¤±çš„é”®"
            ])
        
        if analysis_result.unused_keys:
            suggestions.extend([
                f"â€¢ å‘ç° {len(analysis_result.unused_keys)} ä¸ªæœªä½¿ç”¨çš„å›½é™…åŒ–é”®ï¼Œå¯ä»¥è€ƒè™‘åˆ é™¤ä»¥å‡å°‘æ–‡ä»¶å¤§å°",
                "â€¢ å·²ç”Ÿæˆä¼˜åŒ–åçš„i18næ–‡ä»¶ï¼Œç§»é™¤äº†æœªä½¿ç”¨çš„é”®"
            ])
        
        if analysis_result.inconsistent_keys:
            suggestions.extend([
                f"â€¢ å‘ç° {len(analysis_result.inconsistent_keys)} ä¸ªä¸ä¸€è‡´çš„å›½é™…åŒ–é”®",
                "â€¢ å»ºè®®ä¿æŒæ‰€æœ‰è¯­è¨€æ–‡ä»¶çš„é”®ç»“æ„ä¸€è‡´"
            ])
        
        # è¦†ç›–ç‡å»ºè®®
        if analysis_result.coverage_percentage < 80:
            suggestions.append("â€¢ å½“å‰è¦†ç›–ç‡è¾ƒä½ï¼Œå»ºè®®ä¼˜å…ˆå¤„ç†ç¼ºå¤±çš„é”®")
        elif analysis_result.coverage_percentage < 95:
            suggestions.append("â€¢ è¦†ç›–ç‡è‰¯å¥½ï¼Œå»ºè®®å®Œå–„å‰©ä½™çš„ç¼ºå¤±é”®")
        else:
            suggestions.append("â€¢ è¦†ç›–ç‡ä¼˜ç§€ï¼å»ºè®®å®šæœŸæ£€æŸ¥ä¿æŒä»£ç è´¨é‡")
        
        if not suggestions:
            suggestions.append("â€¢ é¡¹ç›®çš„å›½é™…åŒ–çŠ¶å†µè‰¯å¥½ï¼Œæ²¡æœ‰å‘ç°æ˜æ˜¾é—®é¢˜ï¼")
        
        return suggestions
    
    def _remove_unused_keys(self, data: Dict[str, Any], unused_keys: set) -> Dict[str, Any]:
        """ä»æ•°æ®ä¸­ç§»é™¤æœªä½¿ç”¨çš„é”®"""
        if not isinstance(data, dict):
            return data
        
        result = {}
        for key, value in data.items():
            # æ£€æŸ¥æ˜¯å¦ä¸ºåµŒå¥—ç»“æ„
            if isinstance(value, dict):
                nested_result = self._remove_unused_keys(value, unused_keys)
                if nested_result:  # åªä¿ç•™éç©ºçš„åµŒå¥—å¯¹è±¡
                    result[key] = nested_result
            else:
                # æ„å»ºå®Œæ•´çš„é”®è·¯å¾„
                if key not in unused_keys:
                    result[key] = value
        
        return result
    
    def _set_nested_value(self, data: Dict[str, Any], key_path: str, value: Any):
        """è®¾ç½®åµŒå¥—é”®å€¼"""
        keys = key_path.split('.')
        current = data
        
        for key in keys[:-1]:
            if key not in current:
                current[key] = {}
            current = current[key]
        
        current[keys[-1]] = value
    
    def generate_summary_report(self, analysis_result: AnalysisResult) -> str:
        """
        ç”Ÿæˆç®€è¦æ‘˜è¦æŠ¥å‘Š
        
        Args:
            analysis_result: åˆ†æç»“æœ
            
        Returns:
            str: æ‘˜è¦æŠ¥å‘Šå†…å®¹
        """
        summary_lines = [
            "=" * 40,
            "å›½é™…åŒ–åˆ†ææ‘˜è¦",
            "=" * 40,
            f"ğŸ“Š è¦†ç›–ç‡: {analysis_result.coverage_percentage:.1f}%",
            f"âœ… åŒ¹é…é”®: {analysis_result.matched_keys}/{analysis_result.total_used_keys}",
            f"âŒ ç¼ºå¤±é”®: {len(analysis_result.missing_keys)}",
            f"ğŸ—‘ï¸ æœªä½¿ç”¨é”®: {len(analysis_result.unused_keys)}",
            f"âš ï¸ ä¸ä¸€è‡´é”®: {len(analysis_result.inconsistent_keys)}",
            "=" * 40
        ]
        
        return "\n".join(summary_lines) 
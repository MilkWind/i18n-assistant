"""
æŠ¥å‘Šç”Ÿæˆæ¨¡å— - ç”Ÿæˆåˆ†ææŠ¥å‘Šå’Œä¼˜åŒ–åçš„å›½é™…åŒ–æ–‡ä»¶
"""

import json
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
from dataclasses import asdict

from .analyzer import AnalysisResult, MissingKey, UnusedKey, InconsistentKey
from .parser import ParseResult
from .config import Config


class ReportGenerator:
    """æŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self, config: Config):
        self.config = config
        self.output_path = Path(config.output_path)
        self.output_path.mkdir(parents=True, exist_ok=True)
        self.session_dir = ""  # ä¼šè¯ç›®å½•
    
    def set_session_directory(self, session_dir: str) -> None:
        """è®¾ç½®ä¼šè¯ç›®å½•"""
        self.session_dir = session_dir
    
    def generate_full_report(
        self, 
        analysis_result: AnalysisResult, 
        parse_result: ParseResult
    ) -> str:
        """
        ç”Ÿæˆå®Œæ•´çš„æ–‡æœ¬æŠ¥å‘Š
        
        Args:
            analysis_result: åˆ†æç»“æœ
            parse_result: è§£æç»“æœ
            
        Returns:
            str: æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
        """
        # ç¡®å®šæŠ¥å‘Šè¾“å‡ºè·¯å¾„
        if self.session_dir:
            reports_path = self.output_path / self.session_dir / "reports"
        else:
            reports_path = self.output_path / "reports"
        
        reports_path.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        report_lines = []
        
        # æŠ¥å‘Šå¤´éƒ¨
        report_lines.extend([
            "=" * 60,
            "å›½é™…åŒ–åˆ†ææŠ¥å‘Š",
            "=" * 60,
            f"ç”Ÿæˆæ—¶é—´: {timestamp}",
            f"é¡¹ç›®è·¯å¾„: {self.config.project_path}",
            f"å›½é™…åŒ–ç›®å½•: {self.config.i18n_path}",
            f"è¾“å‡ºç›®å½•: {self.config.output_path}",
            "",
        ])
        
        # æ¦‚è§ˆç»Ÿè®¡
        report_lines.extend([
            "=" * 60,
            "1. æ¦‚è§ˆç»Ÿè®¡",
            "=" * 60,
            f"æ€»ä½¿ç”¨é”®æ•°: {analysis_result.total_used_keys}",
            f"æ€»å®šä¹‰é”®æ•°: {analysis_result.total_defined_keys}",
            f"åŒ¹é…é”®æ•°: {analysis_result.matched_keys}",
            f"è¦†ç›–ç‡: {analysis_result.coverage_percentage:.2f}%",
            f"ç¼ºå¤±é”®æ•°: {len(analysis_result.missing_keys)}",
            f"æœªä½¿ç”¨é”®æ•°: {len(analysis_result.unused_keys)}",
            f"ä¸ä¸€è‡´é”®æ•°: {len(analysis_result.inconsistent_keys)}",
            "",
        ])
        
        # ç¼ºå¤±çš„å›½é™…åŒ–æ–‡æœ¬
        if analysis_result.missing_keys:
            report_lines.extend([
                "=" * 60,
                "2. ç¼ºå¤±çš„å›½é™…åŒ–æ–‡æœ¬",
                "=" * 60,
            ])
            
            # æŒ‰æ–‡ä»¶åˆ†ç»„æ˜¾ç¤ºç»Ÿè®¡æ¦‚è§ˆ
            missing_keys_by_file = getattr(analysis_result, 'missing_keys_by_file', {})
            if missing_keys_by_file:
                report_lines.extend([
                    "",
                    "æ–‡ä»¶ç»Ÿè®¡æ¦‚è§ˆ:",
                    "-" * 30,
                ])
                for file_path, missing_list in missing_keys_by_file.items():
                    report_lines.append(f"  {file_path}: {len(missing_list)} ä¸ªç¼ºå¤±é”®")
            
            report_lines.extend([
                "",
                "è¯¦ç»†åˆ—è¡¨:",
                "-" * 30,
            ])
            
            # æŒ‰æ–‡ä»¶åˆ†ç»„æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            missing_by_file = {}
            for missing in analysis_result.missing_keys:
                if missing.file_path not in missing_by_file:
                    missing_by_file[missing.file_path] = []
                missing_by_file[missing.file_path].append(missing)
            
            for file_path, missing_list in missing_by_file.items():
                report_lines.append(f"\næ–‡ä»¶: {file_path}")
                report_lines.append("-" * 40)
                for missing in missing_list:
                    report_lines.append(f"  è¡Œ {missing.line_number}: '{missing.key}'")
                    if missing.suggested_files:
                        suggestions = ", ".join(missing.suggested_files)
                        report_lines.append(f"    å»ºè®®æ·»åŠ åˆ°: {suggestions}")
        else:
            report_lines.extend([
                "=" * 60,
                "2. ç¼ºå¤±çš„å›½é™…åŒ–æ–‡æœ¬",
                "=" * 60,
                "âœ… æ²¡æœ‰å‘ç°ç¼ºå¤±çš„å›½é™…åŒ–æ–‡æœ¬ï¼",
                "",
            ])
        
        # æœªä½¿ç”¨çš„å›½é™…åŒ–æ–‡æœ¬
        if analysis_result.unused_keys:
            report_lines.extend([
                "",
                "=" * 60,
                "3. æœªä½¿ç”¨çš„å›½é™…åŒ–æ–‡æœ¬",
                "=" * 60,
            ])
            
            # æŒ‰æ–‡ä»¶åˆ†ç»„æ˜¾ç¤ºç»Ÿè®¡æ¦‚è§ˆ
            unused_keys_by_file = getattr(analysis_result, 'unused_keys_by_file', {})
            if unused_keys_by_file:
                report_lines.extend([
                    "",
                    "æ–‡ä»¶ç»Ÿè®¡æ¦‚è§ˆ:",
                    "-" * 30,
                ])
                for file_path, unused_list in unused_keys_by_file.items():
                    report_lines.append(f"  {file_path}: {len(unused_list)} ä¸ªæœªä½¿ç”¨é”®")
            
            report_lines.extend([
                "",
                "è¯¦ç»†åˆ—è¡¨:",
                "-" * 30,
            ])
            
            # æŒ‰æ–‡ä»¶åˆ†ç»„æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            unused_by_file = {}
            for unused in analysis_result.unused_keys:
                if unused.i18n_file not in unused_by_file:
                    unused_by_file[unused.i18n_file] = []
                unused_by_file[unused.i18n_file].append(unused)
            
            for file_path, unused_list in unused_by_file.items():
                report_lines.append(f"\næ–‡ä»¶: {file_path}")
                report_lines.append("-" * 40)
                for unused in unused_list:
                    report_lines.append(f"  '{unused.key}': {unused.value}")
        else:
            report_lines.extend([
                "",
                "=" * 60,
                "3. æœªä½¿ç”¨çš„å›½é™…åŒ–æ–‡æœ¬",
                "=" * 60,
                "âœ… æ²¡æœ‰å‘ç°æœªä½¿ç”¨çš„å›½é™…åŒ–æ–‡æœ¬ï¼",
                "",
            ])
        
        # ä¸ä¸€è‡´çš„å›½é™…åŒ–å­—æ®µ
        if analysis_result.inconsistent_keys:
            report_lines.extend([
                "",
                "=" * 60,
                "4. ä¸ä¸€è‡´çš„å›½é™…åŒ–å­—æ®µ",
                "=" * 60,
            ])
            
            for inconsistent in analysis_result.inconsistent_keys:
                report_lines.append(f"\né”®: '{inconsistent.key}'")
                report_lines.append(f"  å­˜åœ¨äº: {', '.join(inconsistent.existing_files)}")
                report_lines.append(f"  ç¼ºå¤±äº: {', '.join(inconsistent.missing_files)}")
        else:
            report_lines.extend([
                "",
                "=" * 60,
                "4. ä¸ä¸€è‡´çš„å›½é™…åŒ–å­—æ®µ",
                "=" * 60,
                "âœ… æ²¡æœ‰å‘ç°ä¸ä¸€è‡´çš„å›½é™…åŒ–å­—æ®µï¼",
                "",
            ])
        
        # æ–‡ä»¶è¦†ç›–æƒ…å†µ
        if analysis_result.file_coverage:
            report_lines.extend([
                "",
                "=" * 60,
                "5. æ–‡ä»¶è¦†ç›–æƒ…å†µ",
                "=" * 60,
            ])
            
            for file_path, coverage in analysis_result.file_coverage.items():
                report_lines.extend([
                    f"\næ–‡ä»¶: {file_path}",
                    f"  æ€»è°ƒç”¨æ•°: {coverage.total_calls}",
                    f"  è¦†ç›–è°ƒç”¨æ•°: {coverage.covered_calls}",
                    f"  è¦†ç›–ç‡: {coverage.coverage_percentage:.2f}%",
                ])
        
        # å»ºè®®éƒ¨åˆ†
        suggestions = self._generate_suggestions(analysis_result)
        if suggestions:
            report_lines.extend([
                "",
                "=" * 60,
                "6. æ”¹è¿›å»ºè®®",
                "=" * 60,
            ])
            report_lines.extend(suggestions)
        
        # æŠ¥å‘Šå°¾éƒ¨
        report_lines.extend([
            "",
            "=" * 60,
            "æŠ¥å‘Šç»“æŸ",
            "=" * 60,
        ])
        
        # å†™å…¥æ–‡ä»¶
        report_content = "\n".join(report_lines)
        report_file = reports_path / "analysis_report.txt"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        return str(report_file)

    def generate_text_report(self, analysis_result: AnalysisResult) -> str:
        """
        ç”Ÿæˆç®€åŒ–çš„æ–‡æœ¬æŠ¥å‘Š
        
        Args:
            analysis_result: åˆ†æç»“æœ
            
        Returns:
            str: æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
        """
        # ç¡®å®šæŠ¥å‘Šè¾“å‡ºè·¯å¾„
        if self.session_dir:
            reports_path = self.output_path / self.session_dir / "reports"
        else:
            reports_path = self.output_path / "reports"
        
        reports_path.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        summary = self.generate_summary_report(analysis_result)
        
        # å†™å…¥æ–‡ä»¶
        report_file = reports_path / f"summary_report_{timestamp}.txt"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(summary)
        
        return str(report_file)

    def generate_json_report(self, analysis_result: AnalysisResult) -> str:
        """
        ç”ŸæˆJSONæ ¼å¼çš„æŠ¥å‘Š
        
        Args:
            analysis_result: åˆ†æç»“æœ
            
        Returns:
            str: æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
        """
        # ç¡®å®šæŠ¥å‘Šè¾“å‡ºè·¯å¾„
        if self.session_dir:
            reports_path = self.output_path / self.session_dir / "reports"
        else:
            reports_path = self.output_path / "reports"
        
        reports_path.mkdir(parents=True, exist_ok=True)
        
        # æ„å»ºæŠ¥å‘Šæ•°æ®
        report_data = {
            'timestamp': datetime.now().isoformat(),
            'summary': {
                'total_used_keys': analysis_result.total_used_keys,
                'total_defined_keys': analysis_result.total_defined_keys,
                'matched_keys': analysis_result.matched_keys,
                'coverage_percentage': analysis_result.coverage_percentage
            },
            'missing_keys': [asdict(mk) for mk in analysis_result.missing_keys],
            'missing_keys_by_file': {
                file_path: [asdict(mk) for mk in missing_list]
                for file_path, missing_list in getattr(analysis_result, 'missing_keys_by_file', {}).items()
            },
            'missing_keys_summary_by_file': getattr(analysis_result, 'get_missing_keys_summary_by_file', lambda: {})(),
            'unused_keys': [asdict(uk) for uk in analysis_result.unused_keys],
            'unused_keys_by_file': {
                file_path: [asdict(uk) for uk in unused_list]
                for file_path, unused_list in getattr(analysis_result, 'unused_keys_by_file', {}).items()
            },
            'unused_keys_summary_by_file': getattr(analysis_result, 'get_unused_keys_summary_by_file', lambda: {})(),
            'inconsistent_keys': [asdict(ik) for ik in analysis_result.inconsistent_keys],
            'file_coverage': {
                file_path: asdict(coverage) 
                for file_path, coverage in analysis_result.file_coverage.items()
            }
        }
        
        # å†™å…¥JSONæ–‡ä»¶
        json_file = reports_path / "analysis_report.json"
        
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)
        
        return str(json_file)
    
    def generate_missing_keys_template(self, analysis_result: AnalysisResult) -> List[str]:
        """
        ç”Ÿæˆç¼ºå¤±é”®çš„æ¨¡æ¿æ–‡ä»¶
        
        Args:
            analysis_result: åˆ†æç»“æœ
            
        Returns:
            List[str]: ç”Ÿæˆçš„æ¨¡æ¿æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        if not analysis_result.missing_keys:
            return []
        
        generated_files = []
        
        # æŒ‰å»ºè®®çš„æ–‡ä»¶åˆ†ç»„ç¼ºå¤±çš„é”®
        keys_by_suggested_file = {}
        for missing in analysis_result.missing_keys:
            for suggested_file in missing.suggested_files:
                if suggested_file not in keys_by_suggested_file:
                    keys_by_suggested_file[suggested_file] = set()
                keys_by_suggested_file[suggested_file].add(missing.key)
        
        # ä¸ºæ¯ä¸ªå»ºè®®æ–‡ä»¶ç”Ÿæˆæ¨¡æ¿
        for suggested_file, keys in keys_by_suggested_file.items():
            template_data = {}
            
            # ç”Ÿæˆæ¨¡æ¿ç»“æ„
            for key in sorted(keys):
                self._set_nested_value(template_data, key, f"TODO: Add translation for '{key}'")
            
            # åˆ›å»ºæ¨¡æ¿æ–‡ä»¶
            original_path = Path(suggested_file)
            template_file = self.output_path / "templates" / f"missing_keys_{original_path.stem}.json"
            template_file.parent.mkdir(parents=True, exist_ok=True)
            
            with open(template_file, 'w', encoding='utf-8') as f:
                json.dump(template_data, f, ensure_ascii=False, indent=2)
            
            generated_files.append(str(template_file))
        
        return generated_files
    
    def _generate_suggestions(self, analysis_result: AnalysisResult) -> List[str]:
        """ç”Ÿæˆåˆ†æå»ºè®®"""
        suggestions = []
        
        if analysis_result.missing_keys:
            suggestions.extend([
                f"â€¢ å‘ç° {len(analysis_result.missing_keys)} ä¸ªç¼ºå¤±çš„å›½é™…åŒ–é”®ï¼Œå»ºè®®åŠæ—¶æ·»åŠ åˆ°ç›¸åº”çš„i18næ–‡ä»¶ä¸­",
                "â€¢ å¯ä»¥ä½¿ç”¨ç”Ÿæˆçš„æ¨¡æ¿æ–‡ä»¶æ¥å¿«é€Ÿæ·»åŠ ç¼ºå¤±çš„é”®"
            ])
        
        if analysis_result.unused_keys:
            suggestions.extend([
                f"â€¢ å‘ç° {len(analysis_result.unused_keys)} ä¸ªæœªä½¿ç”¨çš„å›½é™…åŒ–é”®ï¼Œå¯ä»¥è€ƒè™‘åˆ é™¤ä»¥å‡å°‘æ–‡ä»¶å¤§å°",
                "â€¢ å·²ç”Ÿæˆä¼˜åŒ–åçš„i18næ–‡ä»¶ï¼Œç§»é™¤äº†æœªä½¿ç”¨çš„é”®"
            ])
        
        if analysis_result.inconsistent_keys:
            suggestions.extend([
                f"â€¢ å‘ç° {len(analysis_result.inconsistent_keys)} ä¸ªä¸ä¸€è‡´çš„å›½é™…åŒ–é”®",
                "â€¢ å»ºè®®ä¿æŒæ‰€æœ‰è¯­è¨€æ–‡ä»¶çš„é”®ç»“æ„ä¸€è‡´"
            ])
        
        # è¦†ç›–ç‡å»ºè®®
        if analysis_result.coverage_percentage < 80:
            suggestions.append("â€¢ å½“å‰è¦†ç›–ç‡è¾ƒä½ï¼Œå»ºè®®ä¼˜å…ˆå¤„ç†ç¼ºå¤±çš„é”®")
        elif analysis_result.coverage_percentage < 95:
            suggestions.append("â€¢ è¦†ç›–ç‡è‰¯å¥½ï¼Œå»ºè®®å®Œå–„å‰©ä½™çš„ç¼ºå¤±é”®")
        else:
            suggestions.append("â€¢ è¦†ç›–ç‡ä¼˜ç§€ï¼å»ºè®®å®šæœŸæ£€æŸ¥ä¿æŒä»£ç è´¨é‡")
        
        if not suggestions:
            suggestions.append("â€¢ é¡¹ç›®çš„å›½é™…åŒ–çŠ¶å†µè‰¯å¥½ï¼Œæ²¡æœ‰å‘ç°æ˜æ˜¾é—®é¢˜ï¼")
        
        return suggestions
    
    def _set_nested_value(self, data: Dict[str, Any], key_path: str, value: Any):
        """è®¾ç½®åµŒå¥—é”®å€¼"""
        keys = key_path.split('.')
        current = data
        
        for key in keys[:-1]:
            if key not in current:
                current[key] = {}
            current = current[key]
        
        current[keys[-1]] = value
    
    def generate_summary_report(self, analysis_result: AnalysisResult) -> str:
        """
        ç”Ÿæˆç®€è¦æ‘˜è¦æŠ¥å‘Š
        
        Args:
            analysis_result: åˆ†æç»“æœ
            
        Returns:
            str: æ‘˜è¦æŠ¥å‘Šå†…å®¹
        """
        summary_lines = [
            "=" * 40,
            "å›½é™…åŒ–åˆ†ææ‘˜è¦",
            "=" * 40,
            f"ğŸ“Š è¦†ç›–ç‡: {analysis_result.coverage_percentage:.1f}%",
            f"âœ… åŒ¹é…é”®: {analysis_result.matched_keys}/{analysis_result.total_used_keys}",
            f"âŒ ç¼ºå¤±é”®: {len(analysis_result.missing_keys)}",
            f"ğŸ—‘ï¸ æœªä½¿ç”¨é”®: {len(analysis_result.unused_keys)}",
            f"âš ï¸ ä¸ä¸€è‡´é”®: {len(analysis_result.inconsistent_keys)}",
        ]
        
        # æ·»åŠ æŒ‰æ–‡ä»¶ç»Ÿè®¡çš„æœªä½¿ç”¨é”®æ‘˜è¦
        unused_keys_by_file = getattr(analysis_result, 'unused_keys_by_file', {})
        if unused_keys_by_file:
            summary_lines.extend([
                "",
                "ğŸ“‚ æœªä½¿ç”¨é”®æŒ‰æ–‡ä»¶ç»Ÿè®¡:",
            ])
            for file_path, unused_list in unused_keys_by_file.items():
                file_name = file_path.split('/')[-1] if '/' in file_path else file_path.split('\\')[-1]
                summary_lines.append(f"   {file_name}: {len(unused_list)} ä¸ª")
        
        summary_lines.append("=" * 40)
        
        return "\n".join(summary_lines) 